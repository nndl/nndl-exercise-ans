{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow2.0 小练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def func(x):\n",
    "    x_exp = tf.math.exp(x)\n",
    "    print(x_exp)\n",
    "    out = x_exp+1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Exp:0\", shape=(), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func(tf.constant(0.1, dtype=tf.float32))\n",
    "func(tf.constant(1., dtype=tf.float32))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 能用到的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.math.exp()\n",
    "tf.math.log()\n",
    "tf.reduce_sum()\n",
    "tf.reduce_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2.7182817  7.389056  20.085537  54.59815  ], shape=(4,), dtype=float32)\n",
      "[ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
      "Tensor(\"Exp:0\", shape=(4,), dtype=float32)\n",
      "[[ 0.04869688  1.03508306  0.58859219 -0.13328246 -0.21204523]\n",
      " [ 0.7523976   0.28543778 -0.28624605 -1.14556942  0.22607593]\n",
      " [-0.24515495  1.2467467   1.40931426  1.83919203 -0.82693547]\n",
      " [-0.19342919 -0.23907839 -2.21973879 -1.32312583  2.36743738]\n",
      " [-0.22354752  0.08595743  1.99159433  1.06231891  0.41089149]\n",
      " [ 0.80098144  1.00561946 -0.71012586  0.08860742  0.1335414 ]\n",
      " [-0.32340475  0.73075424 -0.67997756 -0.52945451 -0.39506264]\n",
      " [-0.23538123 -0.06008378  1.94891277  0.08224466  1.83652073]\n",
      " [-0.76224826 -1.00813168 -0.46573321  0.57136874 -1.02536849]\n",
      " [-0.2479228   1.13761854 -0.44596084 -2.2305736   0.25281442]]\n",
      "----------------------------------------\n",
      "tf.Tensor(\n",
      "[[ 1.32704444]\n",
      " [-0.16790416]\n",
      " [ 3.42316257]\n",
      " [-1.60793481]\n",
      " [ 3.32721464]\n",
      " [ 1.31862386]\n",
      " [-1.19714523]\n",
      " [ 3.57221315]\n",
      " [-2.69011291]\n",
      " [-1.53402429]], shape=(10, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(tf.math.exp([1, 2, 3, 4.]))\n",
    "print(np.exp([1, 2, 3, 4.]))\n",
    "\n",
    "@tf.function\n",
    "def func():\n",
    "    print(tf.math.exp([1, 2, 3, 4.]))\n",
    "func()\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "print(test_data)\n",
    "print('--'*20)\n",
    "print(tf.reduce_sum(test_data, axis=1, keepdims=True)) # [N, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax function: $\\frac{e^{x_i}}{\\sum_{j=1}^{d}{e^{x_j}}}$\n",
    "# $\\sum_{i}{\\frac{e^{x_i}}{\\sum_{j=1}^{d}{e^{x_j}}}} == 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    epsilon = 1e-12\n",
    "    '''shape of x [N, d]'''\n",
    "    x_exp = tf.math.exp(x) # shape=[N, d]\n",
    "    denominator = tf.reduce_sum(x_exp, axis=1, keepdims=True) #shape=[N, 1]\n",
    "    prob_x = x_exp/(denominator+epsilon)\n",
    "    return prob_x\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "(softmax(test_data).numpy() - tf.nn.softmax(test_data, axis=-1).numpy())**2 <0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    '''shape=[N, d]'''\n",
    "    x_exp = tf.math.exp(-x)\n",
    "#     prob_x = 1/(1+x_exp)\n",
    "    prob_x = tf.divide(1, tf.add(1, x_exp))\n",
    "    return prob_x\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "(sigmoid(test_data).numpy() - tf.nn.sigmoid(test_data).numpy())**2 < 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现 softmax 交叉熵loss函数\n",
    "# $\\sum_{c}{-p(y=c|x)*log(p(\\hat{y}=c|x))}$\n",
    "# $s_{ij} = a_{ij}*b_{ij}$ \n",
    "# $s_{ik} = \\sum_j{a_{ij}*b_{jk}}$ matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_ce(p, label):\n",
    "    '''\n",
    "        x shape = [N, c] ===> probability\n",
    "        label shape = [N, c]\n",
    "    '''\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(-label*tf.math.log(p), axis=1))\n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "prob = tf.nn.softmax(test_data)\n",
    "label = np.zeros_like(test_data)\n",
    "label[np.arange(10), np.random.randint(0, 5, size=10)]=1.\n",
    "\n",
    "((tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(label, test_data))\n",
    "  - softmax_ce(prob, label))**2 < 0.0001).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现 sigmoid 交叉熵loss函数\n",
    "\n",
    "# $-p(y=1|x)*log(p(\\hat{y}=1|x)-p(y=0|x)*log(p(\\hat{y}=0|x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid_ce(p, label):\n",
    "    epsilon = 1e-12\n",
    "    '''shape = [N,]'''\n",
    "    losses = -label * tf.math.log(p+epsilon) - (1.-label) * tf.math.log(1.-p+epsilon) # shape=[N,]\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10])\n",
    "prob = tf.nn.sigmoid(test_data)\n",
    "label = np.random.randint(0, 2, 10).astype(test_data.dtype)\n",
    "print (label)\n",
    "\n",
    "((tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(label, test_data))- sigmoid_ce(prob, label))**2 < 0.0001).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.105171>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @tf.function\n",
    "def func(x):\n",
    "    x_exp = tf.math.exp(x)\n",
    "    out = x_exp+1\n",
    "    return out\n",
    "\n",
    "func(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.7182817,  7.389056 , 20.085537 , 54.59815  ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}